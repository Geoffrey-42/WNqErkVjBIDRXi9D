{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47b6f3a4",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c3ae843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as tfl\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "directory = cwd.replace(\"\\\\\", \"/\")[:-9]\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc64c9e9",
   "metadata": {},
   "source": [
    "Define f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e05cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27f4a05",
   "metadata": {},
   "source": [
    "Prepare the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f85493e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 597 files belonging to 2 classes.\n",
      "The class names are: ['flip', 'notflip']\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 40\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "\n",
    "test_dataset = image_dataset_from_directory(directory+\"/images/testing/\",\n",
    "                                            shuffle=True,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            image_size=IMG_SIZE,\n",
    "                                            seed=42)\n",
    "\n",
    "class_names = test_dataset.class_names\n",
    "\n",
    "print(f'The class names are: {class_names}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeae852",
   "metadata": {},
   "source": [
    "Define custom object BottleneckBlock before loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1e518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Bottleneck block from MobileNetV2\n",
    "    \n",
    "    Arguments:\n",
    "    f -- shape of the convolutional window\n",
    "    nb_depth -- number of filters to expand to for the depthwise convolution\n",
    "    nb_project -- number of filters for the projection\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, f, nb_depth, nb_proj, **kwargs):\n",
    "        super(BottleneckBlock, self).__init__(**kwargs)\n",
    "        self.expconv = tfl.Conv2D(filters = nb_depth, \n",
    "                                  kernel_size = (1,1), \n",
    "                                  strides = (1,1), \n",
    "                                  padding = 'same', \n",
    "                                  kernel_initializer = glorot_uniform(seed=42),\n",
    "                                  name = 'expconv'\n",
    "                                  )\n",
    "        self.expbatch = tfl.BatchNormalization(axis = 3, name = 'expbatch')\n",
    "        self.depthconv = tfl.DepthwiseConv2D(kernel_size = (f,f), \n",
    "                                             strides = (1,1), \n",
    "                                             padding = 'same', \n",
    "                                             kernel_initializer = glorot_uniform(seed=42),\n",
    "                                             name = 'depthconv'\n",
    "                                             )\n",
    "        self.depthbatch = tfl.BatchNormalization(axis = 3, name = 'depthbatch')\n",
    "        self.projconv = tfl.Conv2D(filters = nb_proj, \n",
    "                                   kernel_size = (1,1), \n",
    "                                   strides = (1,1), \n",
    "                                   padding = 'same', \n",
    "                                   kernel_initializer = glorot_uniform(seed=42),\n",
    "                                   name = 'projconv'\n",
    "                                   )\n",
    "        self.projbatch = tfl.BatchNormalization(axis = 3, name = 'projbatch')\n",
    "        self.shortbatch = tfl.BatchNormalization(axis = 3, name = 'shortbatch')\n",
    "        self.nb_depth = nb_depth\n",
    "        self.nb_proj = nb_proj\n",
    "        self.f = f\n",
    "    \n",
    "    def call(self, X):\n",
    "        X_shortcut = X # Saved for the shortcut connection\n",
    "    \n",
    "        # Expansion Part\n",
    "        X = self.expconv(X)\n",
    "        X = self.expbatch(X)\n",
    "        X = tfl.Activation('relu')(X)\n",
    "        \n",
    "        # Depthwise Convolution Part\n",
    "        X = self.depthconv(X)\n",
    "        X = self.depthbatch(X)\n",
    "        X = tfl.Activation('relu')(X)\n",
    "        \n",
    "        # Projection Part\n",
    "        X = self.projconv(X)\n",
    "        X = self.projbatch(X)\n",
    "        \n",
    "        # Shortcut Connection\n",
    "        if self.nb_proj == X_shortcut.shape[3]:\n",
    "            X_shortcut = self.shortbatch(X_shortcut)\n",
    "            X = tfl.add([X, X_shortcut])\n",
    "        X = tfl.Activation('relu')(X)\n",
    "    \n",
    "        return X\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(BottleneckBlock, self).get_config()\n",
    "        config.update({\"f\": self.f, \"nb_depth\": self.nb_depth, \"nb_proj\": self.nb_proj})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f878726",
   "metadata": {},
   "source": [
    "Load the model with its custom objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deae8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {\"BottleneckBlock\": BottleneckBlock, \"f1_score\": f1_score}\n",
    "loaded_model = tf.keras.models.load_model('./checkpoints/custom_model.h5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe8e5f",
   "metadata": {},
   "source": [
    "Pick some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b32bebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for images, labels in test_dataset.take(1):\n",
    "    for i in range(BATCH_SIZE):\n",
    "        examples.append(images[i].numpy().astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d7957d",
   "metadata": {},
   "source": [
    "Define a function that gradio app can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb58f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(example):\n",
    "    # Returns a dictionary whose keys are the labels and values their predicted probabilities\n",
    "    prediction = loaded_model.predict(np.reshape(example, (1,) + example.shape))[0][0]\n",
    "    probs = {'not flip':float(prediction), 'flip':float(1-prediction)}\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bcfc11",
   "metadata": {},
   "source": [
    "Generate Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf36df35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interface = gr.Interface(fn=classify_image,\n",
    "                         inputs=gr.Image(shape=(224, 224)),\n",
    "                         outputs=gr.Label(num_top_classes=2),\n",
    "                         description = \"Click on one of the dots below Examples to generate an example\",\n",
    "                         title = \"Flip detector\",\n",
    "                         examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9eafa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://f541056b17bb9c0211.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f541056b17bb9c0211.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interface.launch(share = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
